{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_cnn_with_pretrained_embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "STVavNFnNCEr"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Sentiment Analysis with Keras"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CML_IG6z-iwM",
        "outputId": "82f45259-f21b-4fba-c733-6da74963d08b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "cell_type": "code",
      "source": [
        "# uncomment these for Google collab, will have already been installed in local environment \n",
        "# if 'pip install -r requirements.txt' has been run\n",
        "!pip install nltk\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "from pdb import set_trace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "\n",
        "import glob\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import time"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.106)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.106 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.106)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.106->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FJiWamI00hBp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isdir('./aclImdb'):\n",
        "    if not os.path.isfile('./aclImdb_v1.tar.gz'):\n",
        "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "\n",
        "    if not os.path.isdir('./aclImdb'):  \n",
        "      !tar -xf aclImdb_v1.tar.gz "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U5Tnmoh-Dpfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "time_beginning_of_notebook = time.time()\n",
        "# SAMPLE_SIZE=12500\n",
        "SAMPLE_SIZE=50000 #4000\n",
        "\n",
        "positive_sample_file_list_test = glob.glob(os.path.join('./aclImdb/test/pos', \"*.txt\"))\n",
        "positive_sample_file_list = glob.glob(os.path.join('./aclImdb/train/pos', \"*.txt\"))\n",
        "#positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "negative_sample_file_list_test = glob.glob(os.path.join('./aclImdb/test/neg', \"*.txt\"))\n",
        "negative_sample_file_list = glob.glob(os.path.join('./aclImdb/train/neg', \"*.txt\"))\n",
        "#negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lfr3bXOgXNJJ",
        "outputId": "021c3acf-5df4-4d9f-8efa-3019c992da36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(len(positive_sample_file_list))}) #(SAMPLE_SIZE)\n",
        "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(len(negative_sample_file_list))})\n",
        "\n",
        "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
        "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
        "\n",
        "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
        "\n",
        "df = shuffle(df)\n",
        "\n",
        "##\n",
        "\n",
        "df_positives_test = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list_test], 'sentiment': np.ones(len(positive_sample_file_list))})\n",
        "df_negatives_test = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list_test], 'sentiment': np.zeros(len(negative_sample_file_list))})\n",
        "\n",
        "print(\"Positive review(s)_test:\", df_positives_test['reviews'][1][:100])\n",
        "print(\"Negative review(s)_test:\", df_negatives_test['reviews'][1][:100])\n",
        "\n",
        "df_test = pd.concat([df_positives_test, df_negatives_test], ignore_index=True)\n",
        "\n",
        "df_test = shuffle(df_test)\n",
        "\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
        "X_train, y_train = df['reviews'], df['sentiment']\n",
        "X_test, y_test = df_test['reviews'], df_test['sentiment']\n",
        "\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive review(s): My watch came a little too late but am glad i watched both this and the sequel together...which make\n",
            "Negative review(s): This is one of those movies that appears on cable at like two in the afternoon to entertain bored ho\n",
            "Positive review(s)_test: i see there are great reviews of this film already, i've got a few points to comment on, reasons i t\n",
            "Negative review(s)_test: One would think that with the incredible backdrop of WWII Stalingrad that the writers would come up \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MHZwIv3WfsW6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "73D0MNqQdlID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PA4PuIKtNCGh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ML STUDY GROUP\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "class PreProcessor:\n",
        "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
        "        self.reviews = REVIEWS\n",
        "        self.reviews_val = REVIEWS_VAL\n",
        "        self.labels = LABELS\n",
        "        self.labels_val = LABELS_VAL\n",
        "        self.we_file = WE_FILE\n",
        "\n",
        "    def tokenize(self):\n",
        "#         set_trace()\n",
        "        print(self.reviews[0])\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(self.reviews)\n",
        "\n",
        "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
        "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
        "\n",
        "        self.word_index = tokenizer.word_index\n",
        "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
        "\n",
        "    def make_data(self):\n",
        "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
        "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        \n",
        "        labels = to_categorical(self.labels)\n",
        "        labels_val = to_categorical(self.labels_val)\n",
        "\n",
        "        print(\"Shape of data tensor: \" +str(review.shape))\n",
        "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
        "\n",
        "        return review, review_val, labels, labels_val\n",
        "        \n",
        "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
        "        embeddings_index = {}\n",
        "\n",
        "        if self.we_file == \"rand\":\n",
        "            return None\n",
        "\n",
        "        f = open(self.we_file)\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
        "\n",
        "        for word, i in self.word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                self.embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        return self.embedding_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oU4KPWyIhj1W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VEIPYnFIPMxB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    if not os.path.isfile('./glove.6B.zip'):\n",
        "      !wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "\n",
        "    if not os.path.isfile('./glove.6B.300d.txt'):  \n",
        "      !unzip glove.6B.zip \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Sl6W6_92NCGu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adagrad, Adam\n",
        "from keras import backend as K\n",
        "\n",
        "from pdb import set_trace\n",
        "\n",
        "embedding_dim = 300\n",
        "num_hidden_layers = 3\n",
        "num_hidden_units = 300\n",
        "num_epochs = 100\n",
        "batch_size = 512\n",
        "dropout_rate = 0.2\n",
        "word_dropout_rate = 0.3\n",
        "activation = 'relu'\n",
        "\n",
        "args = {}\n",
        "args['We']='./glove.6B.300d.txt'\n",
        "args['Wels']='' ### rand or ''\n",
        "args['model']='dan'  ### nbow OR dan\n",
        "args['wd']='y'\n",
        "\n",
        "# reviews=X_train.values\n",
        "# reviews_val=X_test.values\n",
        "# labels=y_train.values\n",
        "# labels_val=y_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hcGJe1wVYn6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1083
        },
        "outputId": "d05ddec7-ac3c-4be5-9cf9-bc58477e958b"
      },
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5641    Christ. A sequel to one of the most cloying fi...\n",
              "5335    Supposedly a \"social commentary\" on racism and...\n",
              "6706    We know from other movies that the actors are ...\n",
              "7764    bad acting, bad southern accents, inconsistent...\n",
              "535     The only other film besides Soylent Green that...\n",
              "5027    I have to congratulate the genius who approved...\n",
              "580     Contrary to popular belief, this title , to me...\n",
              "2966    After an anonymous phone call about a spacecra...\n",
              "3516    This was a pretty good film. I'm not sure if t...\n",
              "1184    I don't quite know how to explain \"Darkend Roo...\n",
              "5740    I gave this movie a 5 out of pure pity. My int...\n",
              "2055    Hayao Miyazaki has no equal when it comes to u...\n",
              "6729    Okay, let me break it down for you guys...IT'S...\n",
              "3986    Read Eric's review again. He perfectly describ...\n",
              "3612    After some difficulty, Johnny Yuma arrives at ...\n",
              "173     So, neighbor was killing neighbor. Reminds me ...\n",
              "5264    I saw the movie as a child when it was release...\n",
              "4297    Not sure why this movie seems to have gotten s...\n",
              "2100    Why Panic never got a good theatrical release ...\n",
              "883     Andie MacDowell's facial expressions are great...\n",
              "2957    Joseph L. Mankiewicz is not remembered by most...\n",
              "4463    Simply put, this is the worst movie since \"Pol...\n",
              "7898    As a Native film professor, I can honestly say...\n",
              "5086    Quite possibly the nicest woman in show busine...\n",
              "4907    It's sad when you can see what a movie was att...\n",
              "2911    The performance by Om Puri, Smita Patil, and S...\n",
              "5391    Comparison with American Graffiti is inevitabl...\n",
              "4812    Watching TRUTH ABOUT LOVE (is this a double en...\n",
              "3356    I have to vote this 10 out of 10 in the rare c...\n",
              "2423    This animation has a very simple and straightf...\n",
              "                              ...                        \n",
              "3146    Many people like this movie and many more love...\n",
              "3033    This Wrestlemania just didn't do it for me. Wh...\n",
              "2357    I liked Antz, but loved \"A Bug's Life\". The an...\n",
              "2584    Not a movie for everyone, but this movie is in...\n",
              "7938    I didn't expect much from this TV movie. You h...\n",
              "3331    There I was sitting alone in my flat on a Satu...\n",
              "170     This movie has several things going for it. It...\n",
              "5559    Just saw this at the Chicago Film Festival - a...\n",
              "3153    This excellent series, narrated by Laurence Ol...\n",
              "6232    Beautifully photographed and ably acted, gener...\n",
              "6070    I read all the reviews here AFTER watching thi...\n",
              "2822    Steve Carell once again stars in a light roman...\n",
              "5651    When you have two tower house of performers pi...\n",
              "4267    An Inconvenient Truth is as entirely simplisti...\n",
              "794     Good exciting movie, although it looks to me t...\n",
              "5489    I don't know much about film-making, but good ...\n",
              "6845    Christ, oh Christ... One watches stunned, incr...\n",
              "7002    drss1942 really took the words right out of my...\n",
              "800     I love this show. Now, I'm not a big fan of ma...\n",
              "610     after seeing this film for the 3rd time now i ...\n",
              "1917    Not a bad word to say about this film really. ...\n",
              "6337    As the faux-Russian scientist says two-thirds ...\n",
              "5019    This movie certainly proves, that also the goo...\n",
              "6712    Protocol is an implausible movie whose only sa...\n",
              "5863    I have read the other comment about this movie...\n",
              "5519    What we have here is a downright brilliant pie...\n",
              "7486    \"A total waste of time\" Just throw in a few ex...\n",
              "3783    The only other review of this movie as of this...\n",
              "4113    I think the biggest failing something can have...\n",
              "5885    And, finally, old, old Michael Corleone falls ...\n",
              "Name: reviews, Length: 6000, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OO66Wuy-NCHP",
        "outputId": "af48a4a0-0a79-465e-e6c1-c3b09e9ecb10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "pp = PreProcessor(X_train,X_test,y_train,y_test,args['We'])\n",
        "pp.tokenize()\n",
        "\n",
        "encoded_X_train,encoded_X_test,y_train,y_test = pp.make_data()\n",
        "\n",
        "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a bit slow and boring, the tale of an old man and his wife living a delapidated building and interacting with a fixed cast of characters like the mailman, the brothers sitting on the porch, the wealthy cigar smoking man. The photography of the river is marvelous, as is the interior period decoration. If you like decoration of Banana Republic stores, this is a must.\n",
            "Found 88576 unique tokens\n",
            "self.MAX_SEQUENCE_LENGTH: 2473\n",
            "Shape of data tensor: (25000, 2473)\n",
            "Shape of label tensor: (25000, 2)\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ab3wHsrPNCHp",
        "outputId": "2c832912-fd34-47e2-8edb-6bb6bfce484e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape\n",
        "# pp.MAX_SEQUENCE_LENGTH\n",
        "len(pp.word_index)+1\n",
        "embedding_dim"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OSGu8ksfNCIB"
      },
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XOJqpJ0nNCIj",
        "outputId": "e4ea87c9-de3f-41ed-b03c-40e8bc74ad98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Concatenate\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "# create the model\n",
        "\n",
        "channels = []\n",
        "inputs = []\n",
        "encoded_X_trains= []\n",
        "encoded_X_tests = []\n",
        "for filter_len in [3,4,5]:\n",
        "    inputs1 = Input(shape=(pp.MAX_SEQUENCE_LENGTH,))\n",
        "    inputs.append(inputs1)\n",
        "    embedding1 = Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],\\\n",
        "                           input_length=pp.MAX_SEQUENCE_LENGTH,trainable=True)(inputs1)\n",
        "    conv1 = Conv1D(filters=128, kernel_size=filter_len, padding='same', activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    channels.append(flat1)\n",
        "    encoded_X_trains.append(encoded_X_train)\n",
        "    encoded_X_tests.append(encoded_X_test)\n",
        "    \n",
        "# merge\n",
        "merged = concatenate(channels)\n",
        "# interpretation\n",
        "outputs = Dense(2, activation='softmax')(merged)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "# compile\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 2473, 300)    26573100    input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 2473, 300)    26573100    input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 2473, 300)    26573100    input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 2473, 128)    115328      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 2473, 128)    153728      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 2473, 128)    192128      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 2473, 128)    0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 2473, 128)    0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 2473, 128)    0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 1236, 128)    0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 1236, 128)    0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 1236, 128)    0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 158208)       0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 158208)       0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 158208)       0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 474624)       0           flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            949250      concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 81,129,734\n",
            "Trainable params: 81,129,734\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rxiSQ40iNCI7",
        "outputId": "daa4eb5f-2eab-4f35-c0e2-544f6f74981c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "cell_type": "code",
      "source": [
        "%pdb off\n",
        "batch_size = 64\n",
        "# num_epochs = 3\n",
        "num_epochs = 4\n",
        "\n",
        "history = model.fit(encoded_X_trains,y_train,batch_size=batch_size,epochs=num_epochs,\\\n",
        "          validation_data=(encoded_X_tests,y_test))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned OFF\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/4\n",
            "25000/25000 [==============================] - 209s 8ms/step - loss: 0.4926 - acc: 0.7826 - categorical_accuracy: 0.7826 - val_loss: 0.3012 - val_acc: 0.8764 - val_categorical_accuracy: 0.8764\n",
            "Epoch 2/4\n",
            "25000/25000 [==============================] - 206s 8ms/step - loss: 0.1763 - acc: 0.9346 - categorical_accuracy: 0.9346 - val_loss: 0.3083 - val_acc: 0.8800 - val_categorical_accuracy: 0.8800\n",
            "Epoch 3/4\n",
            "25000/25000 [==============================] - 206s 8ms/step - loss: 0.0724 - acc: 0.9755 - categorical_accuracy: 0.9755 - val_loss: 0.3142 - val_acc: 0.8844 - val_categorical_accuracy: 0.8844\n",
            "Epoch 4/4\n",
            "25000/25000 [==============================] - 207s 8ms/step - loss: 0.0370 - acc: 0.9870 - categorical_accuracy: 0.9870 - val_loss: 0.3756 - val_acc: 0.8826 - val_categorical_accuracy: 0.8826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "S62Y-vjAQPo-"
      },
      "cell_type": "markdown",
      "source": [
        "y_train"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U1L_TrbfNCJQ",
        "outputId": "6ee540bd-9f5d-41b2-9d53-de5e0a86d369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(history.history)\n",
        "df=df[df['val_acc']==df.val_acc.max()]\n",
        "df.reset_index(inplace=True)\n",
        "df[\"title\"]=[\"Keras CNN with pretrained embedding\"]\n",
        "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
        "df[\"nb_epochs\"]=[df.iloc[0][\"index\"]+1]\n",
        "df.drop(labels=\"index\",axis=1,inplace=True)\n",
        "print(df)\n",
        "df.to_csv(path_or_buf=df.iloc[0].title+\".csv\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       acc  categorical_accuracy      loss  val_acc  val_categorical_accuracy  \\\n",
            "0  0.97548               0.97548  0.072373  0.88444                   0.88444   \n",
            "\n",
            "   val_loss                                title  sample_size  nb_epochs  \n",
            "0  0.314249  Keras CNN with pretrained embedding        50000          3  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuOPm6z9KHnJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xz-m2ZgHKv4y",
        "colab_type": "code",
        "outputId": "2810dfb4-be7a-4227-8a19-f56b0cf1e4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc</th>\n",
              "      <th>categorical_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_categorical_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>title</th>\n",
              "      <th>sample_size</th>\n",
              "      <th>nb_epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.97548</td>\n",
              "      <td>0.97548</td>\n",
              "      <td>0.072373</td>\n",
              "      <td>0.88444</td>\n",
              "      <td>0.88444</td>\n",
              "      <td>0.314249</td>\n",
              "      <td>Keras CNN with pretrained embedding</td>\n",
              "      <td>50000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       acc  categorical_accuracy      loss  val_acc  val_categorical_accuracy  \\\n",
              "0  0.97548               0.97548  0.072373  0.88444                   0.88444   \n",
              "\n",
              "   val_loss                                title  sample_size  nb_epochs  \n",
              "0  0.314249  Keras CNN with pretrained embedding        50000          3  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "-dBRudlAKs3G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRw6nHSdD0AM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-pG_hc2xQRMx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
