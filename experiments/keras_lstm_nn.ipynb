{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_lstm_nn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "TP5S5OPti71q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "SAMPLE_SIZE=50000\n",
        "# SAMPLE_SIZE=1000\n",
        "# TRAIN_TEST_RATIO=0.75\n",
        "TRAIN_TEST_RATIO=0.50\n",
        "\n",
        "BATCH_SIZE=64\n",
        "EPOCHS_NB=3\n",
        "# vocab_size = 10000\n",
        "vocab_size = 5000\n",
        "# vocab_size = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CML_IG6z-iwM",
        "outputId": "7c3999c9-0f2b-4063-dc08-b71176bcf61c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "# uncomment these for Google collab, will have already been installed in local environment \n",
        "# if 'pip install -r requirements.txt' has been run\n",
        "!pip install nltk wget\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "\n",
        "import glob\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.115)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.115 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.115)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FJiWamI00hBp",
        "outputId": "53290e05-0961-49d2-ed3c-5ed2ece87412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import tarfile\n",
        "\n",
        "# By checking if the directory exists first, we allow people to delete the tarfile without the notebook re-downloading it\n",
        "if os.path.isdir('aclImdb'):\n",
        "    print(\"Dataset directory exists, taking no action\")\n",
        "else:    \n",
        "    if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "        print(\"Downloading dataset\")\n",
        "        #!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "        wget.download('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
        "    else:\n",
        "        print(\"Dataset already downloaded\")\n",
        "    \n",
        "    print(\"Unpacking dataset\")\n",
        "    #!tar -xf aclImdb_v1.tar.gz \n",
        "    tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "    print(\"Dataset unpacked in aclImdb\")\n",
        "      \n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset directory exists, taking no action\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U5Tnmoh-Dpfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# notebook configuration\n",
        "TRAINING_SET_SIZE=int(SAMPLE_SIZE * (1 - TRAIN_TEST_RATIO))\n",
        "VALIDATION_SET_SIZE=int(SAMPLE_SIZE * TRAIN_TEST_RATIO)\n",
        "\n",
        "dataframes={}\n",
        "for type in [\"train\",\"test\"]:\n",
        "    if type==\"train\":\n",
        "        SLICE = int(TRAINING_SET_SIZE / 2)\n",
        "    elif type==\"test\":\n",
        "        SLICE = int(VALIDATION_SET_SIZE / 2)\n",
        "\n",
        "    positive_file_list = glob.glob(os.path.join('aclImdb/'+type+'/pos', \"*.txt\"))\n",
        "    positive_sample_file_list = positive_file_list[:SLICE]\n",
        "    df_positive = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SLICE)})\n",
        "    \n",
        "    negative_file_list = glob.glob(os.path.join('aclImdb/'+type+'/neg', \"*.txt\"))\n",
        "    negative_sample_file_list = negative_file_list[:SLICE]\n",
        "    df_negative = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SLICE)})\n",
        "\n",
        "    dataframes[type]=pd.concat([df_positive,df_negative])\n",
        "    dataframes[type]=shuffle(dataframes[type])\n",
        "\n",
        "df=pd.concat([dataframes[\"train\"],dataframes[\"test\"]])\n",
        "    \n",
        "X_train=dataframes[\"train\"]['reviews']\n",
        "y_train=dataframes[\"train\"]['sentiment']\n",
        "\n",
        "X_test=dataframes[\"test\"]['reviews']\n",
        "y_test=dataframes[\"test\"]['sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EEMHGH_heeuh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# positive_sample_file_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hwtO6xR4qk-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "q6PQm9-zdZl4",
        "colab_type": "code",
        "outputId": "a1402b8e-5f04-4bfa-8728-42194b48cf9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# X_train.head()\n",
        "X_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fjDcOMEUoToE"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM with Keras (Sequential model)\n",
        "\n",
        "\n",
        "Please note that the below code is executed on GPU instances on Colab, this wont work on your local machine, use the flag to enable/disable running in CPU or GPU mode, set `run_in_GPU_mode_on_colab=false` in order to be able to run in CPU mode."
      ]
    },
    {
      "metadata": {
        "id": "bbzUsjFqdZl9",
        "colab_type": "code",
        "outputId": "b63a8202-9e8a-42ce-a9b1-4f0ce81a2885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# def lstm_keras():\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "tokenize = Tokenizer(num_words=vocab_size)\n",
        "tokenize.fit_on_texts(X_train)\n",
        "\n",
        "encoded_X_train = tokenize.texts_to_sequences(X_train)\n",
        "encoded_X_test = tokenize.texts_to_sequences(X_test)\n",
        "\n",
        "encoded_X_train = sequence.pad_sequences(encoded_X_train, maxlen=vocab_size)\n",
        "encoded_X_test = sequence.pad_sequences(encoded_X_test, maxlen=vocab_size)\n",
        "\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(y_train)\n",
        "encoded_y_train = encoder.transform(y_train)\n",
        "encoded_y_test = encoder.transform(y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fVN5C_34oSgO",
        "outputId": "a4f2548f-d86b-471e-fee2-9928de59d341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 512,input_length=vocab_size))\n",
        "model.add(LSTM(128))  # try using a GRU instead, for fun\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "           optimizer='adam',\n",
        "           metrics=['accuracy'])\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('best.weights', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "callbacks = [model_checkpoint, early_stopping]\n",
        "\n",
        "history = model.fit(encoded_X_train, encoded_y_train, \n",
        "              batch_size=BATCH_SIZE, \n",
        "              epochs=EPOCHS_NB, \n",
        "              verbose=1, \n",
        "              validation_data=(encoded_X_test,encoded_y_test),\n",
        "                   callbacks=callbacks)\n",
        "\n",
        "\n",
        "score = model.evaluate(encoded_X_test, encoded_y_test, \n",
        "                     batch_size=BATCH_SIZE, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 5000, 512)         2560000   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               328192    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,888,321\n",
            "Trainable params: 2,888,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            " 9920/25000 [==========>...................] - ETA: 34:54 - loss: 0.4739 - acc: 0.7714"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oMzC5aJrdZmO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df = history_df[history_df['val_acc']==history_df.val_acc.max()]\n",
        "history_df.reset_index(inplace=True)\n",
        "history_df[\"title\"]=[\"Keras LSTM NN\"]\n",
        "history_df[\"sample_size\"]=[SAMPLE_SIZE]\n",
        "history_df[\"nb_epochs\"]=EPOCHS_NB\n",
        "history_df.drop(labels=\"index\",axis=1,inplace=True)\n",
        "print(history_df)\n",
        "history_df.to_csv(path_or_buf=history_df.iloc[0].title+\".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E9aE_gNldZmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}