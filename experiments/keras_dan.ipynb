{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_dan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "g7nENWrE6ezr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Sentiment Analysis with Keras"
      ]
    },
    {
      "metadata": {
        "id": "mMLDAzVR9OrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "SAMPLE_SIZE=50000\n",
        "# SAMPLE_SIZE=1000\n",
        "# TRAIN_TEST_RATIO=0.75\n",
        "TRAIN_TEST_RATIO=0.50\n",
        "\n",
        "BATCH_SIZE=64\n",
        "EPOCHS_NB=3\n",
        "vocab_size = 10000\n",
        "# vocab_size = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CML_IG6z-iwM",
        "outputId": "c29ddf59-7cb2-4732-bf87-d13d039572b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "cell_type": "code",
      "source": [
        "# uncomment these for Google collab, will have already been installed in local environment \n",
        "# if 'pip install -r requirements.txt' has been run\n",
        "!pip install nltk wget\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "from pdb import set_trace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "\n",
        "import glob\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import time"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.115)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.115 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.115)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.115->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x7-OyL7Z-d1c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    if not os.path.isfile('./glove.6B.zip'):\n",
        "      !wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "\n",
        "    if not os.path.isfile('./glove.6B.300d.txt'):  \n",
        "      !unzip glove.6B.zip \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FJiWamI00hBp",
        "outputId": "ae993487-afbe-40ec-cefe-c8685b5a95cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import tarfile\n",
        "\n",
        "# By checking if the directory exists first, we allow people to delete the tarfile without the notebook re-downloading it\n",
        "if os.path.isdir('aclImdb'):\n",
        "    print(\"Dataset directory exists, taking no action\")\n",
        "else:    \n",
        "    if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "        print(\"Downloading dataset\")\n",
        "        #!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "        wget.download('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
        "    else:\n",
        "        print(\"Dataset already downloaded\")\n",
        "    \n",
        "    print(\"Unpacking dataset\")\n",
        "    #!tar -xf aclImdb_v1.tar.gz \n",
        "    tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "    print(\"Dataset unpacked in aclImdb\")\n",
        "      \n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset directory exists, taking no action\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U5Tnmoh-Dpfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# notebook configuration\n",
        "TRAINING_SET_SIZE=int(SAMPLE_SIZE * (1 - TRAIN_TEST_RATIO))\n",
        "VALIDATION_SET_SIZE=int(SAMPLE_SIZE * TRAIN_TEST_RATIO)\n",
        "\n",
        "dataframes={}\n",
        "for type in [\"train\",\"test\"]:\n",
        "    if type==\"train\":\n",
        "        SLICE = int(TRAINING_SET_SIZE / 2)\n",
        "    elif type==\"test\":\n",
        "        SLICE = int(VALIDATION_SET_SIZE / 2)\n",
        "\n",
        "    positive_file_list = glob.glob(os.path.join('aclImdb/'+type+'/pos', \"*.txt\"))\n",
        "    positive_sample_file_list = positive_file_list[:SLICE]\n",
        "    df_positive = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SLICE)})\n",
        "    \n",
        "    negative_file_list = glob.glob(os.path.join('aclImdb/'+type+'/neg', \"*.txt\"))\n",
        "    negative_sample_file_list = negative_file_list[:SLICE]\n",
        "    df_negative = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SLICE)})\n",
        "\n",
        "    dataframes[type]=pd.concat([df_positive,df_negative])\n",
        "    dataframes[type]=shuffle(dataframes[type])\n",
        "\n",
        "df=pd.concat([dataframes[\"train\"],dataframes[\"test\"]])\n",
        "    \n",
        "X_train=dataframes[\"train\"]['reviews']\n",
        "y_train=dataframes[\"train\"]['sentiment']\n",
        "\n",
        "X_test=dataframes[\"test\"]['reviews']\n",
        "y_test=dataframes[\"test\"]['sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mMhucSi6e0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Logic to compute DAN model"
      ]
    },
    {
      "metadata": {
        "id": "bsbd0HZG6e0C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ML STUDY GROUP\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "class PreProcessor:\n",
        "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
        "        self.reviews = REVIEWS\n",
        "        self.reviews_val = REVIEWS_VAL\n",
        "        self.labels = LABELS\n",
        "        self.labels_val = LABELS_VAL\n",
        "        self.we_file = WE_FILE\n",
        "\n",
        "    def tokenize(self):\n",
        "#         set_trace()\n",
        "        print(self.reviews[0])\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(self.reviews)\n",
        "\n",
        "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
        "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
        "\n",
        "        self.word_index = tokenizer.word_index\n",
        "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
        "\n",
        "    def make_data(self):\n",
        "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
        "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        \n",
        "        labels = to_categorical(self.labels)\n",
        "        labels_val = to_categorical(self.labels_val)\n",
        "\n",
        "        print(\"Shape of data tensor: \" +str(review.shape))\n",
        "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
        "\n",
        "        return review, review_val, labels, labels_val\n",
        "        \n",
        "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
        "        embeddings_index = {}\n",
        "\n",
        "        if self.we_file == \"rand\":\n",
        "            return None\n",
        "\n",
        "        f = open(self.we_file)\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
        "\n",
        "        for word, i in self.word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                self.embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        return self.embedding_matrix\n",
        "\n",
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "class AverageWords(Layer):\n",
        "    def __init__(self):\n",
        "        super(AverageWords,self).__init__()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        axis = K.ndim(x) - 2\n",
        "        if mask is not None:\n",
        "            summed = K.sum(x, axis=axis)\n",
        "            n_words = K.expand_dims(K.sum(K.cast(mask, 'float32'), axis=axis), axis)\n",
        "            return summed / n_words\n",
        "        else:\n",
        "            return K.mean(x, axis=axis)\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return None\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        dimensions = list(input_shape)\n",
        "        n_dimensions = len(input_shape)\n",
        "        del dimensions[n_dimensions - 2]\n",
        "        return tuple(dimensions)\n",
        "\n",
        "class WordDropout(Layer):\n",
        "    def __init__(self, rate):\n",
        "        super(WordDropout,self).__init__()\n",
        "        self.rate = min(1., max(0., rate))\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if 0. < self.rate < 1.0:\n",
        "            def dropped_inputs():\n",
        "                input_shape = K.shape(inputs)\n",
        "                batch_size = input_shape[0]\n",
        "                n_time_steps = input_shape[1]\n",
        "                mask = tf.random_uniform((batch_size, n_time_steps, 1)) >= self.rate\n",
        "                w_drop = K.cast(mask, 'float32') * inputs\n",
        "                return w_drop\n",
        "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
        "        return inputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'rate': self.rate}\n",
        "        base_config = super().get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MSbUF6-x6e0F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adagrad, Adam\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from pdb import set_trace\n",
        "\n",
        "embedding_dim = 300\n",
        "num_hidden_layers = 3\n",
        "num_hidden_units = 300\n",
        "num_epochs = 100\n",
        "batch_size = 512\n",
        "dropout_rate = 0.2\n",
        "word_dropout_rate = 0.3\n",
        "activation = 'relu'\n",
        "\n",
        "args = {}\n",
        "args['We']='./glove.6B.300d.txt'\n",
        "args['Wels']='' ### rand or ''\n",
        "args['model']='dan'  ### nbow OR dan\n",
        "args['wd']='y'\n",
        "\n",
        "reviews=X_train.values\n",
        "reviews_val=X_test.values\n",
        "labels=y_train.values\n",
        "labels_val=y_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "POGvDG_I6e0I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reviews_val.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W7hWELZb6e0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "dec774fb-c51b-4512-b341-b0ea93676981"
      },
      "cell_type": "code",
      "source": [
        "pp = PreProcessor(reviews,reviews_val,labels,labels_val,args['We'])\n",
        "pp.tokenize()\n",
        "\n",
        "reviews,reviews_val,labels,labels_val = pp.make_data()\n",
        "\n",
        "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I'm stunt, I must admit I never saw a movie with such good story and none stop high special effect martial art fighting scene. If you like the fantastic genre, like me, you will certainly be more than satisfied! All character have very cool power and the special effect are near perfection, in one word, flawless! I will listen to this movie a lot in the next years.\n",
            "Found 88576 unique tokens\n",
            "self.MAX_SEQUENCE_LENGTH: 2473\n",
            "Shape of data tensor: (25000, 2473)\n",
            "Shape of label tensor: (25000, 2)\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rERmJWoY6e0T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "if args['Wels'] == \"rand\":\n",
        "    model.add(Embedding(len(pp.word_index) + 1,embedding_dim,input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
        "else:\n",
        "    model.add(Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
        "\n",
        "if args['wd'] == 'y':\n",
        "    model.add(WordDropout(word_dropout_rate))\n",
        "model.add(AverageWords())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SVGnUdxZ6e0W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2f819fe7-b105-4ab0-fed4-92691525ab41"
      },
      "cell_type": "code",
      "source": [
        "print('reviews.shape: ' + str(reviews.shape))\n",
        "print('reviews_val.shape: ' + str(reviews_val.shape))\n",
        "print('labels.shape: ' + str(labels.shape))\n",
        "print('labels_val.shape: ' + str(labels_val.shape))\n",
        "labels.shape[0]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reviews.shape: (25000, 2473)\n",
            "reviews_val.shape: (25000, 2473)\n",
            "labels.shape: (25000, 2)\n",
            "labels_val.shape: (25000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "id": "LsAWW9aj6e0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2128
        },
        "outputId": "a548abe9-3310-4a27-8d94-08e1fdfaa2ac"
      },
      "cell_type": "code",
      "source": [
        "if args['model'] == 'dan':\n",
        "    for i in range(num_hidden_layers):\n",
        "        model.add(Dense(num_hidden_units))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(activation))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "model.add(Dense(labels.shape[1]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "adam = Adam()\n",
        "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy','categorical_accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('best.weights', monitor='val_loss', verbose=1, save_best_only=True)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
        "\n",
        "callbacks = [model_checkpoint, early_stopping]\n",
        "\n",
        "history = model.fit(reviews,labels,batch_size=batch_size,epochs=num_epochs,\\\n",
        "          validation_data=(reviews_val,labels_val), callbacks=callbacks)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 2473, 300)         26573100  \n",
            "_________________________________________________________________\n",
            "word_dropout_3 (WordDropout) (None, 2473, 300)         0         \n",
            "_________________________________________________________________\n",
            "average_words_3 (AverageWord (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 300)               90300     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 2)                 602       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 2)                 8         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 26,848,210\n",
            "Trainable params: 273,306\n",
            "Non-trainable params: 26,574,904\n",
            "_________________________________________________________________\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/100\n",
            "25000/25000 [==============================] - 88s 4ms/step - loss: 0.5965 - acc: 0.6882 - categorical_accuracy: 0.6882 - val_loss: 0.4609 - val_acc: 0.7898 - val_categorical_accuracy: 0.7898\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.46093, saving model to best.weights\n",
            "Epoch 2/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4676 - acc: 0.7808 - categorical_accuracy: 0.7808 - val_loss: 0.5511 - val_acc: 0.7624 - val_categorical_accuracy: 0.7624\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.46093\n",
            "Epoch 3/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4429 - acc: 0.7957 - categorical_accuracy: 0.7957 - val_loss: 0.3783 - val_acc: 0.8318 - val_categorical_accuracy: 0.8318\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.46093 to 0.37829, saving model to best.weights\n",
            "Epoch 4/100\n",
            "25000/25000 [==============================] - 86s 3ms/step - loss: 0.4400 - acc: 0.7987 - categorical_accuracy: 0.7987 - val_loss: 0.3829 - val_acc: 0.8285 - val_categorical_accuracy: 0.8285\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.37829\n",
            "Epoch 5/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4277 - acc: 0.8056 - categorical_accuracy: 0.8056 - val_loss: 0.3805 - val_acc: 0.8283 - val_categorical_accuracy: 0.8283\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.37829\n",
            "Epoch 6/100\n",
            "25000/25000 [==============================] - 86s 3ms/step - loss: 0.4238 - acc: 0.8044 - categorical_accuracy: 0.8044 - val_loss: 0.4102 - val_acc: 0.8077 - val_categorical_accuracy: 0.8077\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.37829\n",
            "Epoch 7/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4175 - acc: 0.8113 - categorical_accuracy: 0.8113 - val_loss: 0.4937 - val_acc: 0.7612 - val_categorical_accuracy: 0.7612\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.37829\n",
            "Epoch 8/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4148 - acc: 0.8119 - categorical_accuracy: 0.8119 - val_loss: 0.3803 - val_acc: 0.8288 - val_categorical_accuracy: 0.8288\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.37829\n",
            "Epoch 9/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4127 - acc: 0.8154 - categorical_accuracy: 0.8154 - val_loss: 0.3720 - val_acc: 0.8341 - val_categorical_accuracy: 0.8341\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.37829 to 0.37197, saving model to best.weights\n",
            "Epoch 10/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4114 - acc: 0.8138 - categorical_accuracy: 0.8138 - val_loss: 0.3752 - val_acc: 0.8313 - val_categorical_accuracy: 0.8313\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.37197\n",
            "Epoch 11/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4098 - acc: 0.8140 - categorical_accuracy: 0.8140 - val_loss: 0.3833 - val_acc: 0.8276 - val_categorical_accuracy: 0.8276\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.37197\n",
            "Epoch 12/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.4043 - acc: 0.8182 - categorical_accuracy: 0.8182 - val_loss: 0.3847 - val_acc: 0.8290 - val_categorical_accuracy: 0.8290\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.37197\n",
            "Epoch 13/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.3974 - acc: 0.8206 - categorical_accuracy: 0.8206 - val_loss: 0.4062 - val_acc: 0.8107 - val_categorical_accuracy: 0.8107\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.37197\n",
            "Epoch 14/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.3981 - acc: 0.8197 - categorical_accuracy: 0.8197 - val_loss: 0.3976 - val_acc: 0.8183 - val_categorical_accuracy: 0.8183\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.37197\n",
            "Epoch 15/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.3973 - acc: 0.8224 - categorical_accuracy: 0.8224 - val_loss: 0.3749 - val_acc: 0.8329 - val_categorical_accuracy: 0.8329\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.37197\n",
            "Epoch 16/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.3906 - acc: 0.8248 - categorical_accuracy: 0.8248 - val_loss: 0.3816 - val_acc: 0.8311 - val_categorical_accuracy: 0.8311\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.37197\n",
            "Epoch 17/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.3943 - acc: 0.8244 - categorical_accuracy: 0.8244 - val_loss: 0.4542 - val_acc: 0.7804 - val_categorical_accuracy: 0.7804\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.37197\n",
            "Epoch 18/100\n",
            "25000/25000 [==============================] - 85s 3ms/step - loss: 0.3949 - acc: 0.8192 - categorical_accuracy: 0.8192 - val_loss: 0.5473 - val_acc: 0.7037 - val_categorical_accuracy: 0.7037\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.37197\n",
            "Epoch 19/100\n",
            "25000/25000 [==============================] - 86s 3ms/step - loss: 0.3853 - acc: 0.8239 - categorical_accuracy: 0.8239 - val_loss: 0.4877 - val_acc: 0.7534 - val_categorical_accuracy: 0.7534\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.37197\n",
            "Epoch 00019: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HbCfwx896e0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a8d6eca9-0ba3-48bf-da9e-c9a387c443b8"
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(history.history)\n",
        "df=df[df['val_acc']==df.val_acc.max()]\n",
        "df.reset_index(inplace=True)\n",
        "df[\"title\"]=[\"Keras DAN\"]\n",
        "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
        "df[\"nb_epochs\"]=[df.iloc[0][\"index\"]+1]\n",
        "df.drop(labels=\"index\",axis=1,inplace=True)\n",
        "print(df)\n",
        "df.to_csv(path_or_buf=df.iloc[0].title+\".csv\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       acc  categorical_accuracy     loss  val_acc  val_categorical_accuracy  \\\n",
            "0  0.81544               0.81544  0.41265  0.83408                   0.83408   \n",
            "\n",
            "   val_loss      title  sample_size  nb_epochs  \n",
            "0  0.371968  Keras DAN        50000          9  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_JT-PqK96e0h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}