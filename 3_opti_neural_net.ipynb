{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_opti_neural_net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "lMofUnI1i05g"
      },
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis of movie (IMDB) reviews using dataset provided by the ACL 2011 paper, \n",
        "see http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "This notebook uses neural net models\n",
        "\n",
        "The plan is to compare a variety of hyperparameters, vectorization techniques, neural net based models:\n",
        "* dense neural network with bag of words\n",
        "* dense neural network with fixed size input and words mapped to integers\n",
        "* LSTM\n",
        "* CNN\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L-FXDJ17i05n"
      },
      "cell_type": "markdown",
      "source": [
        "### Table of Contents<a class=\"anchor\" id=\"table\"></a>\n",
        "* [Load data](#load)\n",
        "* [Train different architectures](#train)\n",
        "    * [Train NN 50 - 10 - 1](#train1)\n",
        "    * [Train NN 256 - 128 - 1](#train2)\n",
        "    * [Train NN with K-Fold cross validation](#kfold)\n",
        "    * [Train RNN](#rnn)\n",
        "* [Optimize](#opti)\n",
        "    * [Optimize on dropouts](#opti_d)\n",
        "        * no dropout\n",
        "        * low dropout on 1 layer\n",
        "        * high dropout on 1 layer\n",
        "        * low dropout on 2 layers\n",
        "        * high dropout on 2 layers\n",
        "        * [Observation](#opti_d_o)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hFAxjxjImFl2",
        "outputId": "50ae4401-3d8a-4263-d54b-a1b0477c092f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eqIsNp2Yi05v",
        "outputId": "e3ea2e5e-cf4c-4aa8-c151-d20357ddff8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "import glob\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tafRpfeyi06R",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x_LeQpbQi06j",
        "outputId": "a7e45b43-d96e-4ac9-8340-f0fec1c05f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import tarfile\n",
        "\n",
        "# By checking if the directory exists first, we allow people to delete the tarfile without the notebook re-downloading it\n",
        "if os.path.isdir('aclImdb'):\n",
        "    print(\"Dataset directory exists, taking no action\")\n",
        "else:    \n",
        "    if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "        print(\"Downloading dataset\")\n",
        "        #!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "        wget.download('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
        "    else:\n",
        "        print(\"Dataset already downloaded\")\n",
        "    \n",
        "    print(\"Unpacking dataset\")\n",
        "    #!tar -xf aclImdb_v1.tar.gz \n",
        "    tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "    print(\"Dataset unpacked in aclImdb\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset directory exists, taking no action\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OCN-uCBMdBJq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def foundGPU():\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    print('No GPU found')\n",
        "    return False\n",
        "  else: \n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PQAl9GvRi067",
        "outputId": "2259ea5d-7391-4eb6-babe-e6318f1a5057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# configuration\n",
        "if foundGPU():\n",
        "    SAMPLE_SIZE=5000\n",
        "# Tried >7000 and the session crashes even before we start training the model which means \n",
        "# we might be doing something wrong with our preparation stage\n",
        "#   SAMPLE_SIZE=12500    \n",
        "else:\n",
        "  SAMPLE_SIZE=2000"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vMYBkcdIB9uc"
      },
      "cell_type": "markdown",
      "source": [
        "<a href='#table'>Back</a>\n",
        "# Load data<a class=\"anchor\" id=\"load\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "pQH5irwZPZuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a dense vector from reviews "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bcTVSZK-i07N",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "time_beginning_of_notebook = time.time()\n",
        "SLICE = int(SAMPLE_SIZE / 2)\n",
        "positive_file_list = glob.glob(os.path.join('aclImdb/train/pos', \"*.txt\"))\n",
        "positive_sample_file_list = positive_file_list[:SLICE]\n",
        "\n",
        "negative_file_list = glob.glob(os.path.join('aclImdb/train/neg', \"*.txt\"))\n",
        "negative_sample_file_list = negative_file_list[:SLICE]\n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dcjl_fogi07W",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "positive_strings = [load_doc(x) for x in positive_sample_file_list]\n",
        "negative_strings = [load_doc(x) for x in negative_sample_file_list]\n",
        "\n",
        "positive_tokenized = [word_tokenize(s) for s in positive_strings]\n",
        "negative_tokenized = [word_tokenize(s) for s in negative_strings]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DrxFWNmFi07d",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TdNEv_DKi07l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_counts = Counter()\n",
        "all_reviews = positive_tokenized + negative_tokenized\n",
        "for r in all_reviews:\n",
        "    for word in r:\n",
        "        total_counts[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IiMud9kdi07u",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = set(total_counts.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-WKETgG_i070",
        "outputId": "71e7c576-a4f9-4042-b69f-1f9b3408d0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I8DCTKf7i078",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a dictionary of words in the vocabulary mapped to index positions\n",
        "# (to be used in layer_0)\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1y_wEoNPi08E",
        "outputId": "e320da9c-34c9-4189-a1d3-d1f1f34e0081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"ID of 'movie' = {}\".format(word2index['movie']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ID of 'movie' = 36827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Rnt7_kXii08M",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_bag(review):\n",
        "    bag = np.zeros(vocab_size)\n",
        "    for word in review:\n",
        "        i = word2index[word]\n",
        "        bag[i]+=1\n",
        "    return bag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BLn_ywWwi08S",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_bag = convert_to_bag(all_reviews[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BWXsLsrTi08X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_reviews_encoded = [convert_to_bag(x) for x in all_reviews]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hlrsPRWWi08f",
        "outputId": "8c025391-b890-4380-dbff-827d7c421e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "all_reviews_encoded[0].shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(53956,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZPEW66FQi08z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "positive_labels = []\n",
        "for i in range(len(positive_tokenized)):\n",
        "    positive_labels.append('POSITIVE')\n",
        "negative_labels = []\n",
        "for i in range(len(negative_tokenized)):\n",
        "    negative_labels.append('NEGATIVE')\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w__rOGc7i085",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = positive_labels + negative_labels\n",
        "\n",
        "num_lables = []\n",
        "\n",
        "for val in labels:\n",
        "    if val == 'POSITIVE':\n",
        "       num_lables.append(1)\n",
        "    else:\n",
        "       num_lables.append(0) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "V75H_F9Gi08-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews_and_labels = list(zip(all_reviews_encoded, num_lables))\n",
        "random.shuffle(reviews_and_labels)\n",
        "reviews, labels = zip(*reviews_and_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HFOEHypri09D",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ExIGdNTQPZvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def saveTrainingMetrics(title):\n",
        "  df = pd.DataFrame(results.history)\n",
        "  df=df[df['val_acc']==df.val_acc.max()]\n",
        "  df.reset_index(inplace=True)\n",
        "  df[\"title\"]=[title]\n",
        "  df[\"sample_size\"]=[SAMPLE_SIZE]\n",
        "  df[\"vocab_size\"]=[vocab_size]  \n",
        "  df[\"nb_epochs\"]=[df.iloc[0][\"index\"]+1]\n",
        "  df.drop(labels=\"index\",axis=1,inplace=True)\n",
        "  print(df)\n",
        "  df.to_csv(path_or_buf=df.iloc[0].title+\".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qeAuh5wcPZvU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a sparse matrix from reviews (where we keep the order of the words)"
      ]
    },
    {
      "metadata": {
        "id": "OpYv1kJXPZvV",
        "colab_type": "code",
        "outputId": "358867cd-10c1-43e6-a130-b36181af597b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "positive_strings[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First, nobody can understand why this movie is rated so poorly. Not only is this the first real horrific movie since a very long time for me who am pretty hard-boiled with a decades long experience of horror starting with driving through dark rides (ghost trains) as a child. Second, the main actress Cheri Christian has a face that lets you hope she will be the leading actress in major pictures of the future. Third, this woman is that tremendously beautiful that I suggest the directors retire all those Cameron Diazes, Eva Mendezes, and how ever the names of these ephemeral bulb-lights are. Mrs. Christian is not a light, but a sun.  However, \"Dark remains\" is also of considerable metaphysical importance. They idea that photographs shows creatures of the intermediary reign between reality and \"imagination\" that are not visible with one\\' own eyes is not new. But I have never seen in a movie before that those creatures are visible on the photographs only for certain people and only to certain times. This means that the photo is not just an iconic picture of reality (by which reality turns into a sign), but becomes an alternative form of reality which can change as the \"real\" reality can. Being a sign, the changing of the picture means that it influences the photographed objects, i.e. the sign behaves like an object. Now, in our usual world of perception, it is common that objects change signs. F.ex., if someone grows a bird, his photograph will show him with beard, not without, as it did before. But the opposite, the changing of objects by signs would imply that the photo with beard is first and only then the beard grows on the man. This is, very simply expressed, the case that happen with the photos taken by the main character in the prison, in this movie. This is new, and we must be thankful for everything new in horror movies which usually just repeat and reorder effects and features that are already well-known, mostly since the silent time.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "WPJRAN45PZvb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### We need to found out why we need this code and possibly who has written this code\n",
        "# reviews=[]\n",
        "# for sentence in positive_strings:\n",
        "#     reviews.append([sentence,1])\n",
        "# for sentence in negative_strings:\n",
        "#     reviews.append([sentence,0])\n",
        "# random.shuffle(reviews)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDQ10eDrPZvf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\\\n",
        "#                                    lower=True, split=' ', char_level=False, oov_token=None, document_count=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LC4R-X_4PZvk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# len(reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEmgoPp-PZvv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a href='#table'>Back</a>\n",
        "# Train models<a class=\"anchor\" id=\"train\"></a>\n",
        "## Train NN 50 - 10 - 1 <a class=\"anchor\" id=\"train1\"></a>\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RsW4fg5wi09R",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(reviews), np.array(labels), test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tpher_taUnSR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "94uBoS1xi09X",
        "outputId": "6e518832-3200-4c00-daf3-41cfe350a2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(vocab_size, )))\n",
        "model.add(layers.Dense(10, activation = \"relu\"))\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 50)                3193400   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                510       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 3,193,921\n",
            "Trainable params: 3,193,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bJDNCzSui09e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YvcEJsRxi09j",
        "outputId": "b1bf491f-17ad-4dc1-92f0-0e8ce689baa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "cell_type": "code",
      "source": [
        "results = model.fit(\n",
        " X_train, y_train,\n",
        " epochs=20,\n",
        " validation_data=(X_test, y_test),\n",
        " batch_size=500\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5250 samples, validate on 1750 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "5250/5250 [==============================] - 3s 631us/sample - loss: 0.6058 - acc: 0.7080 - val_loss: 0.4936 - val_acc: 0.8309\n",
            "Epoch 2/20\n",
            "5250/5250 [==============================] - 3s 480us/sample - loss: 0.3472 - acc: 0.9183 - val_loss: 0.3708 - val_acc: 0.8697\n",
            "Epoch 3/20\n",
            "5250/5250 [==============================] - 3s 482us/sample - loss: 0.1877 - acc: 0.9632 - val_loss: 0.3177 - val_acc: 0.8754\n",
            "Epoch 4/20\n",
            "5250/5250 [==============================] - 3s 484us/sample - loss: 0.1008 - acc: 0.9872 - val_loss: 0.3001 - val_acc: 0.8857\n",
            "Epoch 5/20\n",
            "5250/5250 [==============================] - 3s 482us/sample - loss: 0.0576 - acc: 0.9943 - val_loss: 0.3010 - val_acc: 0.8846\n",
            "Epoch 6/20\n",
            "5250/5250 [==============================] - 3s 480us/sample - loss: 0.0356 - acc: 0.9970 - val_loss: 0.3031 - val_acc: 0.8840\n",
            "Epoch 7/20\n",
            "5250/5250 [==============================] - 3s 481us/sample - loss: 0.0238 - acc: 0.9981 - val_loss: 0.3084 - val_acc: 0.8834\n",
            "Epoch 8/20\n",
            "5250/5250 [==============================] - 3s 479us/sample - loss: 0.0164 - acc: 0.9990 - val_loss: 0.3129 - val_acc: 0.8783\n",
            "Epoch 9/20\n",
            "5250/5250 [==============================] - 3s 480us/sample - loss: 0.0122 - acc: 0.9994 - val_loss: 0.3188 - val_acc: 0.8806\n",
            "Epoch 10/20\n",
            "5250/5250 [==============================] - 3s 481us/sample - loss: 0.0094 - acc: 0.9996 - val_loss: 0.3238 - val_acc: 0.8789\n",
            "Epoch 11/20\n",
            "5250/5250 [==============================] - 3s 479us/sample - loss: 0.0076 - acc: 0.9996 - val_loss: 0.3289 - val_acc: 0.8794\n",
            "Epoch 12/20\n",
            "5250/5250 [==============================] - 3s 482us/sample - loss: 0.0063 - acc: 0.9996 - val_loss: 0.3339 - val_acc: 0.8783\n",
            "Epoch 13/20\n",
            "5250/5250 [==============================] - 3s 478us/sample - loss: 0.0053 - acc: 0.9998 - val_loss: 0.3382 - val_acc: 0.8806\n",
            "Epoch 14/20\n",
            "5250/5250 [==============================] - 2s 475us/sample - loss: 0.0045 - acc: 0.9998 - val_loss: 0.3432 - val_acc: 0.8806\n",
            "Epoch 15/20\n",
            "5250/5250 [==============================] - 3s 485us/sample - loss: 0.0040 - acc: 0.9998 - val_loss: 0.3469 - val_acc: 0.8800\n",
            "Epoch 16/20\n",
            "5250/5250 [==============================] - 3s 480us/sample - loss: 0.0034 - acc: 0.9998 - val_loss: 0.3509 - val_acc: 0.8789\n",
            "Epoch 17/20\n",
            "5250/5250 [==============================] - 3s 482us/sample - loss: 0.0031 - acc: 0.9998 - val_loss: 0.3549 - val_acc: 0.8777\n",
            "Epoch 18/20\n",
            "5250/5250 [==============================] - 3s 484us/sample - loss: 0.0027 - acc: 0.9998 - val_loss: 0.3584 - val_acc: 0.8789\n",
            "Epoch 19/20\n",
            "5250/5250 [==============================] - 3s 480us/sample - loss: 0.0024 - acc: 0.9998 - val_loss: 0.3620 - val_acc: 0.8760\n",
            "Epoch 20/20\n",
            "5250/5250 [==============================] - 3s 480us/sample - loss: 0.0022 - acc: 0.9998 - val_loss: 0.3651 - val_acc: 0.8771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9gpws5-VY17J",
        "colab_type": "code",
        "outputId": "ac898680-83be-43fc-94e6-77faf8453b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "saveTrainingMetrics(\"Opti-NN-Train NN 50 - 10 - 1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        acc     loss   val_acc  val_loss                         title  \\\n",
            "0  0.987238  0.10084  0.885714  0.300149  Opti-NN-Train NN 50 - 10 - 1   \n",
            "\n",
            "   sample_size  vocab_size  nb_epochs  \n",
            "0         7000       63867          4  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vPb3tdEfPZv9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train NN 256 - 128 - 1 <a class=\"anchor\" id=\"train2\"></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IG7nratoi09w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
        "    layers.Dense(128, activation = \"relu\"),\n",
        "    layers.Dense(1, activation = \"sigmoid\")\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IOI6rAd_i091",
        "outputId": "521faa3d-c947-4a1b-c90b-3e2a2ddda465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 256)               16350208  \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 16,383,233\n",
            "Trainable params: 16,383,233\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tHpN5VM8i097",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        " optimizer = \"adam\",\n",
        " loss = \"binary_crossentropy\",\n",
        " metrics = [\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FhcPKZnFi0-D",
        "outputId": "42ed6f6e-264e-4970-b134-def9623d12e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "results = model.fit(\n",
        " X_train, y_train,\n",
        " epochs= 1,\n",
        " validation_data=(X_test, y_test),\n",
        "batch_size=500\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5250 samples, validate on 1750 samples\n",
            "5250/5250 [==============================] - 3s 597us/sample - loss: 0.6035 - acc: 0.6979 - val_loss: 0.4399 - val_acc: 0.8343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "i2T7Tom0i0-O",
        "outputId": "5d3dc92e-80b4-494b-ae1f-12d52258efa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "results.history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'acc': [0.69790477],\n",
              " 'loss': [0.6035325229167938],\n",
              " 'val_acc': [0.83428574],\n",
              " 'val_loss': [0.43990913884980337]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "X3N7EaCHYHgJ",
        "colab_type": "code",
        "outputId": "1871342c-fe3f-4211-8c3b-c7d53bb017e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "saveTrainingMetrics(\"Opti-NN-Train NN 256 - 128 - 1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        acc      loss   val_acc  val_loss                           title  \\\n",
            "0  0.697905  0.603533  0.834286  0.439909  Opti-NN-Train NN 256 - 128 - 1   \n",
            "\n",
            "   sample_size  vocab_size  nb_epochs  \n",
            "0         7000       63867          1  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C1QyhP2mPZwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train NN with K-Fold cross validation <a class=\"anchor\" id=\"kfold\"></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_LwIgaq-i0-a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(reviews), np.array(labels), test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "c60zERWRi0-e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kfold = KFold(3, True, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OR8fKYxai0-j",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = list(zip(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pV3wG31Qi0-o",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data[0][0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5sw1JTMli0-u",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "histories=[]\n",
        "for train_indices, test_indices in kfold.split(X_train,y=y_train):\n",
        "    model = keras.Sequential([\n",
        "    layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
        "    layers.Dense(128, activation = \"relu\"),\n",
        "    layers.Dense(1, activation = \"sigmoid\")\n",
        "    ])\n",
        "    model.compile(\n",
        "     optimizer = \"adam\",\n",
        "     loss = \"binary_crossentropy\",\n",
        "     metrics = [\"accuracy\"]\n",
        "    )\n",
        "    K_X_train = X_train[train_indices]\n",
        "    K_y_train = y_train[train_indices]\n",
        "    K_X_test = X_train[test_indices]\n",
        "    K_y_test = y_train[test_indices]\n",
        "    results=model.fit(\n",
        "        K_X_train, K_y_train,\n",
        "        epochs= 5,\n",
        "        validation_data=(K_X_test, K_y_test),\n",
        "        batch_size=128\n",
        "    )\n",
        "    histories.append(results.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "swLxj14uYQT5",
        "colab_type": "code",
        "outputId": "5065f0df-53cc-45cf-a89a-298d52396acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "saveTrainingMetrics(\"Opti-NN-Train NN with K-Fold cross validation\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      acc      loss  val_acc  val_loss  \\\n",
            "0  0.9828  0.135673   0.8648   0.34739   \n",
            "\n",
            "                                           title  sample_size  vocab_size  \\\n",
            "0  Opti-NN-Train NN with K-Fold cross validation         5000       53956   \n",
            "\n",
            "   nb_epochs  \n",
            "0          5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "s5ouE4n7i0-3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.DataFrame(data=histories)\n",
        "for col in df.columns:\n",
        "    df[col] =  df[col].apply(lambda x: x[-1])\n",
        "plot=df[[\"acc\",\"val_acc\"]].plot()\n",
        "plot.set_ylim([0,1])\n",
        "\n",
        "means=df[[\"acc\",\"val_acc\"]].mean()\n",
        "print(\"mean acc: {}, mean val_acc: {}\".format(means[\"acc\"],means[\"val_acc\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MhsXqWp2PZw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train RNN <a class=\"anchor\" id=\"rnn\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "hIVA8Xr9PZw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rDReAbCPZxA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a href='#table'>Back</a>\n",
        "# OPTIMIZE<a class=\"anchor\" id=\"opti\"></a>"
      ]
    },
    {
      "metadata": {
        "id": "14G1v5TEPZxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimize on dropout<a class=\"anchor\" id=\"opti_d\"></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wn6ISenGq7Aa",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(reviews), np.array(labels), test_size=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nzc1nQ7wq7Am",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kfold = KFold(3, True, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "V2iZzqKVq7Ax",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = list(zip(X_train, y_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8VNkRI_f36lv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pdb import set_trace\n",
        "\n",
        "def getMeansFromResultsHistory(histories):\n",
        "  df = pd.DataFrame(data=histories)\n",
        "  for col in df.columns:\n",
        "      df[col] =  df[col].apply(lambda x: x[-1])\n",
        "  means=df[[\"acc\",\"val_acc\"]].mean()\n",
        "  return means\n",
        "\n",
        "def trainModelWithDropoutOn1Layer(epochs_nb=5,rate=0.0):\n",
        "  histories=[]\n",
        "  for train_indices, test_indices in kfold.split(X_train,y=y_train):\n",
        "      model = keras.Sequential([\n",
        "      layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
        "      layers.Dropout(rate),\n",
        "      layers.Dense(128, activation = \"relu\"),\n",
        "      layers.Dense(1, activation = \"sigmoid\")\n",
        "      ])\n",
        "      model.compile(\n",
        "       optimizer = \"adam\",\n",
        "       loss = \"binary_crossentropy\",\n",
        "       metrics = [\"accuracy\"]\n",
        "      )\n",
        "      K_X_train = X_train[train_indices]\n",
        "      K_y_train = y_train[train_indices]\n",
        "      K_X_test = X_train[test_indices]\n",
        "      K_y_test = y_train[test_indices]\n",
        "      results=model.fit(\n",
        "          K_X_train, K_y_train,\n",
        "          epochs= epochs_nb,\n",
        "          validation_data=(K_X_test, K_y_test),\n",
        "          batch_size=1000\n",
        "      )\n",
        "      histories.append(results.history)\n",
        "#       set_trace()\n",
        "  \n",
        "  means=getMeansFromResultsHistory(histories)\n",
        "  print(means) \n",
        "  return results,means\n",
        "\n",
        "\n",
        "def trainModelWithDropoutOn2Layers(epochs_nb=5,rate=0.0):\n",
        "  histories=[]\n",
        "  for train_indices, test_indices in kfold.split(X_train,y=y_train):\n",
        "      model = keras.Sequential([\n",
        "      layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
        "      layers.Dropout(rate),\n",
        "      layers.Dense(128, activation = \"relu\"),\n",
        "      layers.Dropout(rate),\n",
        "      layers.Dense(1, activation = \"sigmoid\")\n",
        "      ])\n",
        "      model.compile(\n",
        "       optimizer = \"adam\",\n",
        "       loss = \"binary_crossentropy\",\n",
        "       metrics = [\"accuracy\"]\n",
        "      )\n",
        "      K_X_train = X_train[train_indices]\n",
        "      K_y_train = y_train[train_indices]\n",
        "      K_X_test = X_train[test_indices]\n",
        "      K_y_test = y_train[test_indices]\n",
        "      results=model.fit(\n",
        "          K_X_train, K_y_train,\n",
        "          epochs= epochs_nb,\n",
        "          validation_data=(K_X_test, K_y_test),\n",
        "          batch_size=1000\n",
        "      )\n",
        "      histories.append(results.history)\n",
        "#       set_trace()\n",
        "  \n",
        "  means=getMeansFromResultsHistory(histories)\n",
        "  print(means) \n",
        "  return results,means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9s4XxkFy2gaU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dropout_means=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUGkh0VEPZxR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### No dropout\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jOicSA33xPyy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "37ed6826-f4cf-4fe6-a09c-2cc0e84e5b37"
      },
      "cell_type": "code",
      "source": [
        "rate=0.0\n",
        "results,means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
        "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 639us/sample - loss: 0.6808 - acc: 0.5704 - val_loss: 0.6432 - val_acc: 0.6960\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 476us/sample - loss: 0.5131 - acc: 0.8496 - val_loss: 0.5145 - val_acc: 0.7696\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 483us/sample - loss: 0.3129 - acc: 0.9352 - val_loss: 0.4295 - val_acc: 0.8184\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 478us/sample - loss: 0.1704 - acc: 0.9716 - val_loss: 0.3919 - val_acc: 0.8400\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 474us/sample - loss: 0.0893 - acc: 0.9888 - val_loss: 0.3646 - val_acc: 0.8552\n",
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 642us/sample - loss: 0.7129 - acc: 0.5056 - val_loss: 0.6507 - val_acc: 0.5568\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 476us/sample - loss: 0.5382 - acc: 0.7736 - val_loss: 0.5219 - val_acc: 0.8200\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 477us/sample - loss: 0.3501 - acc: 0.9568 - val_loss: 0.4224 - val_acc: 0.8424\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 477us/sample - loss: 0.1952 - acc: 0.9808 - val_loss: 0.3691 - val_acc: 0.8616\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 473us/sample - loss: 0.1031 - acc: 0.9900 - val_loss: 0.3511 - val_acc: 0.8600\n",
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 648us/sample - loss: 0.6896 - acc: 0.5376 - val_loss: 0.6390 - val_acc: 0.6800\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 477us/sample - loss: 0.5018 - acc: 0.8672 - val_loss: 0.5107 - val_acc: 0.8136\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 474us/sample - loss: 0.3144 - acc: 0.9568 - val_loss: 0.4196 - val_acc: 0.8328\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 478us/sample - loss: 0.1737 - acc: 0.9772 - val_loss: 0.3689 - val_acc: 0.8448\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.0919 - acc: 0.9884 - val_loss: 0.3336 - val_acc: 0.8624\n",
            "acc        0.989067\n",
            "val_acc    0.859200\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iADhPy0UaJet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "cbf01259-1f0c-4d6b-cad6-3c4109a1698f"
      },
      "cell_type": "code",
      "source": [
        "saveTrainingMetrics(\"Opti-NN-Optimise - No dropout\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      acc      loss  val_acc  val_loss                          title  \\\n",
            "0  0.9884  0.091937   0.8624  0.333578  Opti-NN-Optimise - No dropout   \n",
            "\n",
            "   sample_size  vocab_size  nb_epochs  \n",
            "0         5000       53956          5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TnO39h8BPZxW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Low dropout on 1 layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q5uhaJ-_rOO2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "ce7235ba-0ec3-4cac-d2f7-db76f7609143"
      },
      "cell_type": "code",
      "source": [
        "rate=0.2\n",
        "results,means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
        "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,1])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 663us/sample - loss: 0.6885 - acc: 0.5356 - val_loss: 0.6566 - val_acc: 0.6448\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 491us/sample - loss: 0.5436 - acc: 0.8168 - val_loss: 0.5365 - val_acc: 0.7528\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 478us/sample - loss: 0.3543 - acc: 0.9180 - val_loss: 0.4420 - val_acc: 0.8144\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 478us/sample - loss: 0.2070 - acc: 0.9652 - val_loss: 0.3749 - val_acc: 0.8440\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 477us/sample - loss: 0.1151 - acc: 0.9824 - val_loss: 0.3543 - val_acc: 0.8568\n",
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 672us/sample - loss: 0.7339 - acc: 0.5176 - val_loss: 0.6854 - val_acc: 0.5160\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.6256 - acc: 0.6200 - val_loss: 0.5885 - val_acc: 0.8000\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 478us/sample - loss: 0.4840 - acc: 0.9304 - val_loss: 0.4929 - val_acc: 0.8208\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 480us/sample - loss: 0.3377 - acc: 0.9384 - val_loss: 0.4181 - val_acc: 0.8384\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 477us/sample - loss: 0.2096 - acc: 0.9716 - val_loss: 0.3666 - val_acc: 0.8640\n",
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 680us/sample - loss: 0.7038 - acc: 0.5328 - val_loss: 0.6290 - val_acc: 0.6336\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.5312 - acc: 0.8236 - val_loss: 0.5236 - val_acc: 0.8024\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 484us/sample - loss: 0.3531 - acc: 0.9320 - val_loss: 0.4268 - val_acc: 0.8368\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 478us/sample - loss: 0.2040 - acc: 0.9704 - val_loss: 0.3591 - val_acc: 0.8576\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.1087 - acc: 0.9844 - val_loss: 0.3286 - val_acc: 0.8656\n",
            "acc        0.979467\n",
            "val_acc    0.862133\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tue_UCkXaA2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "b4893496-d612-49ff-d213-ce098376e84e"
      },
      "cell_type": "code",
      "source": [
        "saveTrainingMetrics(\"Opti-NN-Optimise - Low dropout on 1 layer\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      acc      loss  val_acc  val_loss  \\\n",
            "0  0.9844  0.108719   0.8656  0.328614   \n",
            "\n",
            "                                       title  sample_size  vocab_size  \\\n",
            "0  Opti-NN-Optimise - Low dropout on 1 layer         5000       53956   \n",
            "\n",
            "   nb_epochs  \n",
            "0          5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1CrRZgpaPZxZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### High dropout on 1 layer\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1CblTzT_uul1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "fcb6693b-0953-48cc-fd64-836f3603d3af"
      },
      "cell_type": "code",
      "source": [
        "rate=0.4\n",
        "results,means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
        "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 688us/sample - loss: 0.7076 - acc: 0.5292 - val_loss: 0.6660 - val_acc: 0.6048\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 483us/sample - loss: 0.6176 - acc: 0.7212 - val_loss: 0.5910 - val_acc: 0.7624\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.4786 - acc: 0.8880 - val_loss: 0.4995 - val_acc: 0.7888\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.3323 - acc: 0.9208 - val_loss: 0.4216 - val_acc: 0.8264\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.2163 - acc: 0.9536 - val_loss: 0.3728 - val_acc: 0.8440\n",
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 698us/sample - loss: 0.7165 - acc: 0.5212 - val_loss: 0.6603 - val_acc: 0.5624\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 474us/sample - loss: 0.6174 - acc: 0.6984 - val_loss: 0.5829 - val_acc: 0.8048\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 475us/sample - loss: 0.4742 - acc: 0.9060 - val_loss: 0.4674 - val_acc: 0.8240\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 476us/sample - loss: 0.3154 - acc: 0.9420 - val_loss: 0.3984 - val_acc: 0.8408\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 481us/sample - loss: 0.1939 - acc: 0.9576 - val_loss: 0.3583 - val_acc: 0.8608\n",
            "Train on 2500 samples, validate on 1250 samples\n",
            "Epoch 1/5\n",
            "2500/2500 [==============================] - 2s 706us/sample - loss: 0.7298 - acc: 0.5076 - val_loss: 0.6657 - val_acc: 0.6440\n",
            "Epoch 2/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.6357 - acc: 0.6836 - val_loss: 0.6134 - val_acc: 0.7152\n",
            "Epoch 3/5\n",
            "2500/2500 [==============================] - 1s 486us/sample - loss: 0.5301 - acc: 0.8552 - val_loss: 0.5346 - val_acc: 0.7968\n",
            "Epoch 4/5\n",
            "2500/2500 [==============================] - 1s 479us/sample - loss: 0.3971 - acc: 0.9128 - val_loss: 0.4495 - val_acc: 0.8296\n",
            "Epoch 5/5\n",
            "2500/2500 [==============================] - 1s 477us/sample - loss: 0.2727 - acc: 0.9488 - val_loss: 0.3823 - val_acc: 0.8512\n",
            "acc        0.953333\n",
            "val_acc    0.852000\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jBOeTGYQZ3Jf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "da97d719-e929-4cdf-9a13-71bdfc9fc9d2"
      },
      "cell_type": "code",
      "source": [
        "saveTrainingMetrics(\"Opti-NN-Optimise - High dropout on 1 layer\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      acc      loss  val_acc  val_loss  \\\n",
            "0  0.9488  0.272705   0.8512  0.382278   \n",
            "\n",
            "                                        title  sample_size  vocab_size  \\\n",
            "0  Opti-NN-Optimise - High dropout on 1 layer         5000       53956   \n",
            "\n",
            "   nb_epochs  \n",
            "0          5  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "19DRpJToPZxd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Low dropout on 2 layers"
      ]
    },
    {
      "metadata": {
        "id": "YndqkNTkPZxi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### High dropout on 2 layers\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gz8yOkHOzD0k"
      },
      "cell_type": "markdown",
      "source": [
        "Plot results"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kWtdXNAAvW_c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data=dropout_means,columns=['acc','val_acc','rate','nb_layers'])\n",
        "plt.rcParams[\"figure.figsize\"] = [17,2]\n",
        "plot=df[[\"acc\",\"val_acc\"]].plot()\n",
        "plot.set_ylim([0.7,1])\n",
        "plot.grid()\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [17,1.5]\n",
        "plot=df[[\"rate\"]].plot()\n",
        "plot.set_ylim([0,0.5])\n",
        "plot.grid()\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [17,1.5]\n",
        "plot=df[[\"nb_layers\"]].plot()\n",
        "plot.set_ylim([0,2.2])\n",
        "plot.grid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "46vBFEGrzpgG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dropout_means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UKxdXoGZPZxx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Observation<a class=\"anchor\" id=\"opti_d_o\"></a>\n",
        "we have similar results, but got a higher test accuracy with low dropout on all layers and also less overfit (training and test accuracies are closer)"
      ]
    }
  ]
}